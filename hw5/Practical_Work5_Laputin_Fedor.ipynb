{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/falaputin2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "def load_glove_vectors(fn):\n",
    "    print(\"Loading Glove Model\")\n",
    "    with open(fn, \"r\", encoding=\"utf8\") as glove_vector_file:\n",
    "        model = {}\n",
    "        for line in glove_vector_file:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            embedding = np.array([float(val) for val in parts[1:]])\n",
    "            model[word] = embedding\n",
    "        print(\"Loaded {} words\".format(len(model)))\n",
    "    return model\n",
    "\n",
    "\n",
    "glove_vectors = load_glove_vectors(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentencesInArticle</th>\n",
       "      <th>WordsInSentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Humraaz () is a 2002 Indian Hindi-language mu...</td>\n",
       "      <td>[[humraaz, is, a, indian, hindilanguage, music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yehoshua Glazer (29 December 1927 - 29 Decemb...</td>\n",
       "      <td>[[yehoshua, glazer, december, december, was, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[On 27 October 2018, an AgustaWestland AW169 h...</td>\n",
       "      <td>[[on, october, an, agustawestland, aw, helicop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Randolph County is a county in the U.S. state...</td>\n",
       "      <td>[[randolph, county, is, a, county, in, the, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The Real Adventures of Jonny Quest is an Amer...</td>\n",
       "      <td>[[the, real, adventures, of, jonny, quest, is,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  SentencesInArticle  \\\n",
       "0  [Humraaz () is a 2002 Indian Hindi-language mu...   \n",
       "1  [Yehoshua Glazer (29 December 1927 - 29 Decemb...   \n",
       "2  [On 27 October 2018, an AgustaWestland AW169 h...   \n",
       "3  [Randolph County is a county in the U.S. state...   \n",
       "4  [The Real Adventures of Jonny Quest is an Amer...   \n",
       "\n",
       "                                    WordsInSentences  \n",
       "0  [[humraaz, is, a, indian, hindilanguage, music...  \n",
       "1  [[yehoshua, glazer, december, december, was, a...  \n",
       "2  [[on, october, an, agustawestland, aw, helicop...  \n",
       "3  [[randolph, county, is, a, county, in, the, yo...  \n",
       "4  [[the, real, adventures, of, jonny, quest, is,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = (\n",
    "    pd.read_csv(\"./small_wikidump.csv\")\n",
    "    .sample(10, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "CLEAN_PATTERN = r\"[^a-zA-z\\s]\"\n",
    "\n",
    "\n",
    "def clean(word):\n",
    "    return re.sub(CLEAN_PATTERN, \"\", word)\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = [clean(word) for word in sentence]\n",
    "    if len(sentence) == 0:\n",
    "        print(\"empty\")\n",
    "        sentence = [\"PAD\"]\n",
    "    return [word for word in sentence if word]\n",
    "\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "    return [clean_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def lower(sentence):\n",
    "    return [word.lower() for word in sentence]\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    words = [word for word in sentence if word not in stop_words]\n",
    "    return [word for word in words if len(word) > 0]\n",
    "\n",
    "\n",
    "def tokenize_words(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def fix_contractions(sentences):\n",
    "    return [contractions.fix(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def remove_n(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def pad_empty(word_senteces):\n",
    "    new_word_sentences = []\n",
    "    for word_sentece in word_senteces:\n",
    "        if len(word_sentece) == 0:\n",
    "            new_word_sentences.append([\"PAD\"])\n",
    "        else:\n",
    "            new_word_sentences.append(word_sentece)\n",
    "    return new_word_sentences\n",
    "\n",
    "\n",
    "articles[\"text\"] = articles.text.apply(remove_n)\n",
    "articles[\"SentencesInArticle\"] = articles.text.apply(sent_tokenize)\n",
    "articles[\"WordsInSentences\"] = (\n",
    "    articles.SentencesInArticle.apply(fix_contractions)\n",
    "    .apply(lower)\n",
    "    .apply(tokenize_words)\n",
    "    .apply(remove_stopwords)\n",
    "    .apply(clean_sentences)\n",
    ")\n",
    "\n",
    "articles[\"WordsInSentences\"] = articles[\"WordsInSentences\"].apply(pad_empty)\n",
    "articles = articles[[\"SentencesInArticle\", \"WordsInSentences\"]]\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 50\n",
    "EMPTY_VECTOR = np.zeros(VECTOR_SIZE)\n",
    "\n",
    "\n",
    "def sentence_vector(sentence):\n",
    "    return sum([glove_vectors.get(word, EMPTY_VECTOR) for word in sentence]) / len(\n",
    "        sentence\n",
    "    )\n",
    "\n",
    "\n",
    "def sentences_to_vectors(sentences):\n",
    "    return [sentence_vector(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def similarity_matrix(sentence_vectors):\n",
    "    sim_mat = np.zeros([len(sentence_vectors), len(sentence_vectors)])\n",
    "    for i in range(len(sentence_vectors)):\n",
    "        for j in range(len(sentence_vectors)):\n",
    "            element_i = sentence_vectors[i].reshape(1, VECTOR_SIZE)\n",
    "            element_j = sentence_vectors[j].reshape(1, VECTOR_SIZE)\n",
    "            sim_mat[i][j] = cosine_similarity(element_i, element_j)[0, 0]\n",
    "    return sim_mat\n",
    "\n",
    "\n",
    "def compute_graph(sim_matrix):\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return scores\n",
    "\n",
    "\n",
    "articles[\"SentenceVector\"] = articles.WordsInSentences.apply(sentences_to_vectors)\n",
    "articles[\"SimMatrix\"] = articles.SentenceVector.apply(similarity_matrix)\n",
    "articles[\"Graph\"] = articles.SimMatrix.apply(compute_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_sentences(sentences, scores, n=3):\n",
    "    top_scores = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    top_n_sentences = [sentence for score, sentence in top_scores[:n]]\n",
    "    return \" \".join(top_n_sentences)\n",
    "\n",
    "\n",
    "articles[\"Summary\"] = articles.apply(\n",
    "    lambda d: get_ranked_sentences(d.SentencesInArticle, d.Graph), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film is loosely based on the 1998 film A Perfect Murder. Humraaz () is a 2002 Indian Hindi-language musical romantic thriller film directed by the duo Abbas-Mustan. This film received positive reviews and was extremely successful at the box office.\n",
      "He was born in Tel Aviv. Glazer died on his 91st birthday on 29 December 2018 in Tel Aviv. He played for Maccabi Tel Aviv and for the Israel national football team his entire career.\n",
      "Club owner Vichai Srivaddhanaprabha was on board, as well as two other passengers and two pilots. On 27 October 2018, an AgustaWestland AW169 helicopter crashed shortly after take-off from the King Power Stadium, the home ground of Leicester City in Leicester, United Kingdom. The Air Accidents Investigation Branch is currently leading an investigation into the accident.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(articles.loc[i].Summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
